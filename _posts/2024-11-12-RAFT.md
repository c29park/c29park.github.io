## Retrieval Augmented Fine Tuning (RAFT)

Here's an interesting take on RAG and fine-tuning. [This paper](https://arxiv.org/pdf/2403.10131) combines the two; specifically, it sticks to domain specific RAG and does fine-tuning on top of that. 

---------------------------------------------

### Analogy on Fine-tuning
<img width="958" height="550" alt="image" src="https://github.com/user-attachments/assets/dd591036-f133-46b5-ae15-87f41f00ef31" />

The paper addresses fine-tuning as a closed-book exam, where a student walks into the test site and answers questions without an access to an external material only from their memory. 

### Analogy on RAG
<img width="964" height="572" alt="image" src="https://github.com/user-attachments/assets/2d623dd6-3589-4a73-96a3-85faaf6a8a8f" />

On the other hand, the paper addresses RAG as an open-book exam, where a student is allowed to view external sources but has not seen the sources before. Therefore, the student does not have a prior knowledge of the tested material.

### RAFT analogy
<img width="970" height="552" alt="image" src="https://github.com/user-attachments/assets/76b026b7-5ac1-4697-84d1-0ab249cedb8e" />


Now RAFT is a domain specific open-book exam, but I think cheating on a closed-book exam fits better for the analogy. 
The student enters what's supposed to be an unknown open-book exam, but the student has coaxed his professor to reveal the testing material to him and brings a cheating material with him secretly to the testing site.

RAFT teaches the model to use external docs or test while having baked in knowledge at train time, thereby solving the issue of lack of external information for fine-tuning and the answer quality dependency on the external documents. 

### Idea

The idea is to use Supervised Fine-Tuning (SFT) during train time and domain specific RAG on test time. 
For the training data, p% of data is $$Q+ D^o+D_2 + ... + D_k --> A$$ and the rest (1-p)% of data is $$Q+D_1+D_2+ ... + D_k --> A$$

where 
+ $$D_k$$ = set of distractor documents that contain no answer relevant information
+ $$D^o$$ = set of Oracle Docs (documents from which the answer to the question can be deduced)
+ A = set of corresponding CoT style answers generated from one of the Oracle Docs

The reason why they add distractor documents and oracle documents (golden and important) is that once trained on this dataset, the language model can inherently distinguish irrelevant information and relevant information. 
The trained LLM then performs Top-k document retrievals via RAG and then answers given a query. 
Here's an example training data:

<img width="658" height="318" alt="image" src="https://github.com/user-attachments/assets/e784bc5e-7a47-45d1-8011-4eba3f1cc7fe" />

They also performed experiments for the hyperparameter p to see what value is optimal. p is the fraction of the training data that contains the oracle document(s).

<img width="968" height="234" alt="image" src="https://github.com/user-attachments/assets/e87e1c62-ccc6-4b69-ba7b-20de17665f43" />

As seen, the more relevant documents you include in training, the less the model would rely on the relevant external source. This underscores the importance of adding distractor documents. 
